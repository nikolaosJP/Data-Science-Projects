{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h3>Introduction to Style transferring</h3>\n\n- Neural style transfer (NST) is an optimization technique used to take two images; a content image and a style reference image (such as an artwork by a famous painter) and blend them together so the output image looks like the content image, but “painted” in the style of the style reference image.\n\n- Training a style transfer model requires two networks: a pre-trained feature extractor and a transfer network. An NST uses a pre-trained model trained on ImageNet or VGG. These try to minimize either the content loss (which help to establish similarities between the content image and the generated image) or style loss which is given by a Gram matrix which is a square matrix, with a width and height equal to the depth of the convolutional layer in question. \n\n<h3>The goal of this Kernel</h3>\n\n- This body of work represents a small chunk of code inspired by Udacity's \"Intro to Deep Learning with PyTorch\" which anyone can use to transform an image into a piece of art (no painting skills required!).","metadata":{}},{"cell_type":"markdown","source":"<h4>Step 1.1 - Import Modules</h4>","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torch.optim as optim\nimport requests\nfrom torchvision import transforms, models","metadata":{"execution":{"iopub.status.busy":"2022-05-15T07:22:34.480096Z","iopub.execute_input":"2022-05-15T07:22:34.480610Z","iopub.status.idle":"2022-05-15T07:22:34.491367Z","shell.execute_reply.started":"2022-05-15T07:22:34.480550Z","shell.execute_reply":"2022-05-15T07:22:34.490099Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"<h4>Step 1.2 - Load in the pre-trained VGG19 model (specifically its features)</h4>\n\nVGG19 is split into two portions:\n- vgg19.features which are all the convolutional and pooling layers\n- vgg19.classifier which are the three linear, classifier layers at the end\n\nWe only need the features portion, which we're going to load in and \"freeze\" the weights of, below.","metadata":{}},{"cell_type":"code","source":"# Loading the model\nvgg = models.vgg19(pretrained=True).features\n\n# freeze all VGG parameters since we're only optimizing the target image\nfor param in vgg.parameters():\n    param.requires_grad_(False)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T07:22:34.497330Z","iopub.execute_input":"2022-05-15T07:22:34.497860Z","iopub.status.idle":"2022-05-15T07:22:36.415611Z","shell.execute_reply.started":"2022-05-15T07:22:34.497820Z","shell.execute_reply":"2022-05-15T07:22:36.414805Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"<h4>Step 1.3 - Move the model to the Graphics Processing Unit (GPU) if one is available</h4>","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# if this results in \"device(type='cuda')\" then a GPU is indeed available\ndevice\n\n# Moving the model to the GPU\nvgg.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T07:22:36.417153Z","iopub.execute_input":"2022-05-15T07:22:36.417430Z","iopub.status.idle":"2022-05-15T07:22:36.450378Z","shell.execute_reply.started":"2022-05-15T07:22:36.417374Z","shell.execute_reply":"2022-05-15T07:22:36.449435Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<h4>Step 1.4 - Load in the content and style images through a function</h4>","metadata":{}},{"cell_type":"code","source":"def load_image(img_path, max_size=400, shape=None):\n    \n    #Load in and transform an image, making sure the image is <= 400 pixels in the x-y dims.\n    if 'http' in img_path:\n        response = requests.get(img_path)\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    else:\n        image = Image.open(img_path).convert('RGB')\n        \n    # large images will slow down processing\n    if max(image.size) > max_size:\n        size = max_size\n    else:\n        size = max(image.size)\n    \n    if shape is not None:\n        size = shape\n    \n    in_transform = transforms.Compose([transforms.Resize(size),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.485, 0.456, 0.406), \n                                                            (0.229, 0.224, 0.225))])\n    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n    image = in_transform(image)[:3,:,:].unsqueeze(0)\n    \n    return image\n       \n\n# Loading the content image\ncontent = load_image('../input/photos/chinese_p.jpg').to(device)\n# Loading the style image, resizing it to match the content image\nstyle = load_image('../input/photos/vang_1.png', shape=content.shape[-2:]).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T07:22:36.451991Z","iopub.execute_input":"2022-05-15T07:22:36.452272Z","iopub.status.idle":"2022-05-15T07:22:36.534775Z","shell.execute_reply.started":"2022-05-15T07:22:36.452234Z","shell.execute_reply":"2022-05-15T07:22:36.534034Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"<h4>Step 2 - Visualizing the images</h4>\n\nHere, we are going to build a function for un-normalizing the images and converting them from a Tensor image to a NumPy image for display","metadata":{}},{"cell_type":"code","source":"def im_convert(tensor):\n    \"\"\" Display a tensor as an image. \"\"\"\n    \n    image = tensor.to(\"cpu\").clone().detach()\n    image = image.numpy().squeeze()\n    image = image.transpose(1,2,0)\n    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n    image = image.clip(0, 1)\n\n    return image\n\n# display the images\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\n# content and style ims side-by-side\nax1.imshow(im_convert(content))\nax2.imshow(im_convert(style))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T07:22:36.536957Z","iopub.execute_input":"2022-05-15T07:22:36.537243Z","iopub.status.idle":"2022-05-15T07:22:37.125547Z","shell.execute_reply.started":"2022-05-15T07:22:36.537207Z","shell.execute_reply":"2022-05-15T07:22:37.123027Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"<h4>Step 3.1 - Selecting the content and style features </h4>\n\nTo get the content and style representations of an image, we have to pass an image forward through the VGG19 network until we get to the desired layer(s) and then get the output from that layer.\n\nThis is achieved as follows:","metadata":{}},{"cell_type":"code","source":"# Content and Style Features\ndef get_features(image, model, layers=None):\n    \"\"\" Run an image forward through a model and get the features for \n        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n    \"\"\"\n    ## Need the layers for the content and style representations of an image\n    if layers is None:\n        layers = {'0': 'conv1_1',\n                  '5': 'conv2_1',\n                  '10': 'conv3_1',\n                  '19': 'conv4_1',\n                  '21': 'conv4_2',\n                  '28': 'conv5_1'}\n        \n        \n    ## -- do not need to change the code below this line -- ##\n    features = {}\n    x = image\n    # model._modules is a dictionary holding each module in the model\n    for name, layer in model._modules.items():\n        x = layer(x)\n        if name in layers:\n            features[layers[name]] = x\n            \n    return features","metadata":{"execution":{"iopub.status.busy":"2022-05-15T07:22:37.126888Z","iopub.execute_input":"2022-05-15T07:22:37.127309Z","iopub.status.idle":"2022-05-15T07:22:37.134468Z","shell.execute_reply.started":"2022-05-15T07:22:37.127273Z","shell.execute_reply":"2022-05-15T07:22:37.133848Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"<h4> 3.2 Designing the Gram Matrix </h4>\n\nAs briefly touched upon earlier, a gram matrix is the result of multiplying a given matrix by its transposed matrix. In this application the given matrix is a reshaped version of the image feature maps.\nXL of a layer LL.","metadata":{}},{"cell_type":"code","source":"def gram_matrix(tensor):\n    \"\"\" Calculate the Gram Matrix of a given tensor \n        Gram Matrix: https://en.wikipedia.org/wiki/Gramian_matrix\n    \"\"\"\n    \n    # get the batch_size, depth, height, and width of the Tensor\n    b, d, h, w = tensor.size()\n    \n    # reshape so we're multiplying the features for each channel\n    tensor = tensor.view(b * d, h * w)\n    \n    # calculate the gram matrix\n    gram = torch.mm(tensor, tensor.t())\n    \n    return gram ","metadata":{"execution":{"iopub.status.busy":"2022-05-15T07:22:37.135834Z","iopub.execute_input":"2022-05-15T07:22:37.136332Z","iopub.status.idle":"2022-05-15T07:22:37.145865Z","shell.execute_reply.started":"2022-05-15T07:22:37.136294Z","shell.execute_reply":"2022-05-15T07:22:37.144855Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"<h4> 3.3 Putting everything together </h4>\n\nWe'll extract our features from our images and calculate the gram matrices for each layer in our style representation.","metadata":{}},{"cell_type":"code","source":"# get content and style features only once before training\ncontent_features = get_features(content, vgg)\nstyle_features = get_features(style, vgg)\n\n# calculate the gram matrices for each layer of our style representation\nstyle_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n\n# create a third \"target\" image and prep it for change it is a good idea to start off with the target as a copy of our *content* image then iteratively change its style\ntarget = content.clone().requires_grad_(True).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-15T07:22:37.147487Z","iopub.execute_input":"2022-05-15T07:22:37.147981Z","iopub.status.idle":"2022-05-15T07:22:42.751479Z","shell.execute_reply.started":"2022-05-15T07:22:37.147941Z","shell.execute_reply":"2022-05-15T07:22:42.750643Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"<h4> 4.1 Setting model loss and weights </h4>\n\n In this section, we will set the weights for each style layer. Remember that weighting earlier layers more will result in *larger* style artifacts. Also, notice that we are excluding `conv4_2` which is  our content representation.","metadata":{}},{"cell_type":"code","source":"# Model weights\nstyle_weights = {'conv1_1': 1,\n                 'conv2_1': 0.9,\n                 'conv3_1': 0.6,\n                 'conv4_1': 0.65,\n                 'conv5_1': 0.7}\n\n# you may choose to leave these as is\ncontent_weight = 5 # alpha\nstyle_weight = 1e4 # beta","metadata":{"execution":{"iopub.status.busy":"2022-05-15T07:22:42.752906Z","iopub.execute_input":"2022-05-15T07:22:42.753178Z","iopub.status.idle":"2022-05-15T07:22:42.757766Z","shell.execute_reply.started":"2022-05-15T07:22:42.753126Z","shell.execute_reply":"2022-05-15T07:22:42.757001Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"<h4> 4.2 Updating the target and caclulating the losses </h4>","metadata":{}},{"cell_type":"code","source":"# for displaying the target image, intermittenly \nshow_every = 8000\n\n# iteration hyperparameters\noptimizer = optim.Adam([target], lr=0.01)\nsteps = 40000 # decide how many iterations to update your image\n\nfor ii in range(1, steps+1):\n    # get the features from your target image, then calculate the content loss\n    target_features = get_features(target, vgg)\n    # the content loss\n    content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n    # the style loss\n    # initialize the style loss to 0\n    style_loss = 0\n    # iterate through each style layer and add to the style loss\n    for layer in style_weights:\n        # get the \"target\" style representation for the layer\n        target_feature = target_features[layer]\n        _, d, h, w = target_feature.shape\n        # Calculate the target gram matrix\n        target_gram = gram_matrix(target_feature)\n        # get the \"Style\" style representation\n        style_gram = style_grams[layer]\n        # Calculate the style loss for one layer, weighted appropriately\n        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram) ** 2)\n        # add to the style loss\n        style_loss += layer_style_loss / (d * h * w)\n    # Calculate the total loss\n    total_loss = (content_loss * content_weight) + (style_loss * style_weight)\n    \n    # update the target image\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n    \n    # display intermediate images and print the loss\n    if ii % show_every == 0:\n        print('Total loss: ', total_loss.item())\n        plt.imshow(im_convert(target))\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-15T07:22:42.759310Z","iopub.execute_input":"2022-05-15T07:22:42.759824Z","iopub.status.idle":"2022-05-15T07:57:48.355611Z","shell.execute_reply.started":"2022-05-15T07:22:42.759783Z","shell.execute_reply":"2022-05-15T07:57:48.354957Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"<h4> 4.3 Display the target image </h4>","metadata":{}},{"cell_type":"code","source":"# display content and final, target image\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 10))\nax1.imshow(im_convert(content))\nax2.imshow(im_convert(target))\nax3.imshow(im_convert(style))","metadata":{"execution":{"iopub.status.busy":"2022-05-15T07:57:48.357746Z","iopub.execute_input":"2022-05-15T07:57:48.360301Z","iopub.status.idle":"2022-05-15T07:57:49.003745Z","shell.execute_reply.started":"2022-05-15T07:57:48.360261Z","shell.execute_reply":"2022-05-15T07:57:49.003103Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"<h4> Final remarks</h4>\n\n- As we can see, this piece of code can successfully mix traditional Chinese (shan shui) and post-impressionism styles to create one remarkable piece of art.\n\n- Feel free to mix and match any content image with any style of your preference, though if you choose to do so, make sure to update the hyperparameters (number of iterations, learning rate, model weights, etc.) in order to create a model suitable for your own needs.","metadata":{}}]}